import codecs  # used for writing files - more unicode friendly than standard open() module

import shutil
import sys
import os
currentdir = os.curdir

from multiprocessing import Pool
import multiprocessing
mp_logger = multiprocessing.log_to_stderr()
mp_logger.setLevel(25)
from itertools import repeat

from time import time

import iron_horse
import utils
import global_constants


def run_vehicle_pipelines(target_config, graphics_output_path):
    pipelines = target_config["default_model_variant"].gestalt_graphics.pipelines
    if len(pipelines) == 0:
        raise Exception("no pipelines")
    else:
        # run pipelines, obvs
        for pipeline in pipelines:
            pipeline.render(target_config, global_constants, graphics_output_path)


def run_spritelayer_cargo_set_pipelines(
    spritelayer_cargo_set_pair, graphics_output_path
):
    for pipeline in spritelayer_cargo_set_pair[
        0
    ].gestalt_graphics.spritelayer_cargo_pipelines:
        pipeline.render(
            spritelayer_cargo_set_pair, global_constants, graphics_output_path
        )


def report_sprites_complete(catalogues):
    # project management eh :P
    total = len(catalogues)
    complete_count = sum(
        1 for catalogue in catalogues if catalogue.factory.model_def.sprites_complete
    )
    percent_complete = int(100 * (complete_count / total))

    logger.info(
        f"Sprites complete for {complete_count} vehicles; "
        f"incomplete for {total - complete_count} vehicles; {percent_complete}%"
    )

    incomplete_by_track_type = {}
    for catalogue in catalogues:
        if not catalogue.factory.model_def.sprites_complete:
            track_type = catalogue.factory.model_def.base_track_type_name
            incomplete_by_track_type.setdefault(track_type, []).append(catalogue)

    for track_type, group in incomplete_by_track_type.items():
        logger.info(f"  * {track_type} {len(group)}")


# wrapped in a main() function so this can be called explicitly, because unexpected multiprocessing fork bombs are bad
def main():
    globals()['logger'] = utils.get_logger(__file__)
    logger.info(f"[RENDER GRAPHICS] {' '.join(sys.argv)}")
    start = time()
    iron_horse.main()
    # get args passed by makefile
    command_line_args = utils.get_command_line_args()
    # default to no mp, makes debugging easier (mp fails to pickle errors correctly)
    num_pool_workers = command_line_args.num_pool_workers
    if num_pool_workers == 0:
        use_multiprocessing = False
        # just print, no need for a coloured echo_message
        logger.info("Multiprocessing disabled: (PW=0)")
    else:
        use_multiprocessing = True
        # just print, no need for a coloured echo_message
        logger.info("Multiprocessing enabled: (PW={str(num_pool_workers)})")

    roster = iron_horse.roster_manager.active_roster
    # expect Exception failures if there is no active roster, don't bother explicitly handling that case

    graphics_input_path = os.path.join(currentdir, "src", "graphics")
    graphics_output_path = os.path.join(
        iron_horse.generated_files_path, "graphics", roster.grf_name
    )
    # tmp dir used for file comparison of output, which is used to only write over the actual graphics file if it has changed
    # otherwise the nml sprite cache is undesirably invalidated for files where only the timestamp has changed
    graphics_output_path_tmp = os.path.join(
        graphics_output_path,
        "tmp",
    )
    # exist_ok=True is used for case with parallel make (`make -j 2` or similar), don't fail with error if dir already exists
    os.makedirs(graphics_output_path, exist_ok=True)
    # always forcibly remove and recreate the tmp dir to remove any stale files from e.g. failed compiles
    if os.path.exists(graphics_output_path_tmp):
        shutil.rmtree(graphics_output_path_tmp, ignore_errors=True)
    os.makedirs(graphics_output_path_tmp)

    hint_file = codecs.open(
        os.path.join(graphics_output_path, "_graphics_files_here_are_generated.txt"),
        "w",
        "utf8",
    )
    hint_file.write(
        "Don't edit the graphics files here. They're generated by the build script. \n Edit sources in src/graphics."
    )
    hint_file.close()

    # get a list of 2-tuple pairs for spritelayer cargos + cargo sets
    # a list format is wanted for convenience with graphics multiprocessing pool
    # the parent spritelayer_cargo object must be passed with the cargo set as cargo sets have render-time properties which change according to context
    # but cargo_sets are global and reused across spritelayer_cargos, so they can't just store a single reference to their spritelayer_cargo parent
    spritelayer_cargo_set_pairs = []
    for spritelayer_cargo in iron_horse.registered_spritelayer_cargos:
        for cargo_set in spritelayer_cargo.cargo_sets:
            spritelayer_cargo_set_pairs.append((spritelayer_cargo, cargo_set))

    # sort the catalogues into render passes, pass 1 will be first
    # this enables some vehicles to depend on generated sprites from ealier render passes
    render_pass_targets = {1: [], 2: []}
    for catalogue in roster.catalogues:
        default_model_variant = catalogue.default_model_variant_from_roster
        target_config = {
            "catalogue": catalogue,
            "default_model_variant": default_model_variant,
        }
        render_pass_targets[
            default_model_variant.gestalt_graphics.render_pass_num
        ].append(target_config)

    if use_multiprocessing == False:
        for spritelayer_cargo_set_pair in spritelayer_cargo_set_pairs:
            run_spritelayer_cargo_set_pipelines(
                spritelayer_cargo_set_pair, graphics_output_path
            )
        for render_pass_num in [1, 2]:
            for target_config in render_pass_targets[render_pass_num]:
                run_vehicle_pipelines(target_config, graphics_output_path)
    else:
        # we only need to create the pool once
        pool = Pool(processes=num_pool_workers)
        # vehicle pool jobs are organised into passes so that some vehicles can depend on generated sprites from others
        # starmap is blocking not async, which is what we want here
        pool.starmap(
            run_spritelayer_cargo_set_pipelines,
            zip(
                spritelayer_cargo_set_pairs,
                repeat(graphics_output_path),
            ),
        )
        for render_pass in [1, 2]:
            pool.starmap(
                run_vehicle_pipelines,
                zip(
                    render_pass_targets[render_pass],
                    repeat(graphics_output_path),
                ),
            )
        pool.close()
        pool.join()

    report_sprites_complete(roster.catalogues)

    for dir_name in ["railtypes", "signals", "tail_lights"]:
        target_path = os.path.join(graphics_input_path, dir_name)
        dest_path = os.path.join(graphics_output_path, dir_name)
        if os.path.exists(dest_path):
            shutil.rmtree(dest_path)
        shutil.copytree(target_path, dest_path)

    logger.info(
        f"[RENDER GRAPHICS]"
        f"{command_line_args.grf_name} - complete "
        f"{utils.string_format_compile_time_deltas(start, time())}"
    )


if __name__ == "__main__":
    main()
